# ğŸ•¸ï¸ Web Crawler (Full Breakdown)

---

## ğŸ“Œ Problem Prompt

> **"Design and implement a web crawler that starts from a given seed URL and crawls all reachable URLs within the same domain."**
>
> Youâ€™ll be provided a helper function like:

```python
def get_urls(url: str) -> List[str]:
    """Returns a list of links (strings) found on the given URL page."""
```

> Youâ€™ll be asked to:

* Write a **single-threaded** crawler first.
* Then make it **multi-threaded or async**.
* Then answer **production system design** follow-ups.

---

## ğŸ§  Constraints & Clarifications

* **Only crawl URLs in the same domain** as the seed URL.
* **Ignore fragments** (`#section`).
* You donâ€™t need to handle normalization like `http:// vs https://`.
* Replit or an online environment will be used â€” be ready to print logs.
* URLs may appear multiple times â€” donâ€™t revisit.
* They may provide a **wrapper around jsoup** (in Java) or equivalent.

---

## âœ… Step-by-Step Implementation

---

### ğŸ”¹ Step 1 â€“ Basic Single-threaded Crawler (DFS)

```python
from urllib.parse import urlparse
from typing import Set, List

def web_crawler(start_url: str, get_urls) -> Set[str]:
    seen = set()
    domain = urlparse(start_url).netloc

    def dfs(url):
        if url in seen:
            return
        if urlparse(url).netloc != domain:
            return
        seen.add(url)
        for next_url in get_urls(url):
            if '#' in next_url:  # filter fragment
                next_url = next_url.split('#')[0]
            dfs(next_url)

    dfs(start_url)
    return seen
```

---

### ğŸ”¹ Step 2 â€“ Convert to Multi-threaded Crawler (ThreadPool)

```python
import threading
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
from typing import Set

def web_crawler_multithreaded(start_url: str, get_urls) -> Set[str]:
    seen = set()
    seen_lock = threading.Lock()
    domain = urlparse(start_url).netloc
    executor = ThreadPoolExecutor(max_workers=10)

    def crawl(url):
        with seen_lock:
            if url in seen:
                return
            seen.add(url)
        if urlparse(url).netloc != domain:
            return

        for next_url in get_urls(url):
            if '#' in next_url:
                next_url = next_url.split('#')[0]
            executor.submit(crawl, next_url)

    crawl(start_url)
    executor.shutdown(wait=True)
    return seen
```

---

## ğŸ§ª Real Interview Follow-Up Questions + Answers

---

### ğŸ” Q1: **How would you implement a politeness policy (rate-limiting)?**

> â€œOur current crawler is too aggressive.â€

#### âœ… Answer:

* Use a **per-domain rate limiter**:

  * Sleep `X` milliseconds between requests.
  * Use a `domain â†’ last_request_time` dictionary.
* Better: use a **token bucket** or `asyncio.Semaphore`.

Example (simplified):

```python
import time

last_fetch = {}
def polite_fetch(url):
    domain = urlparse(url).netloc
    now = time.time()
    if domain in last_fetch and now - last_fetch[domain] < 1:
        time.sleep(1 - (now - last_fetch[domain]))
    last_fetch[domain] = time.time()
    return get_urls(url)
```

---

### ğŸ” Q2: **How would you scale this to millions of URLs across many machines?**

> â€œå•æœºæ‰›ä¸ä½äº†æ€ä¹ˆåŠï¼Ÿâ€

#### âœ… Answer:

* Use **distributed crawling architecture**:

  * Frontend or CLI submits seed URLs.
  * Store unvisited URLs in a shared queue (e.g. Redis, Kafka).
  * Each worker:

    * Pulls URL from queue.
    * Crawls and extracts new URLs.
    * Pushes new URLs back into the queue if unseen.
  * Use **Bloom filters** or sharded databases for deduplication.

---

### ğŸ” Q3: **How would you detect and avoid duplicate/similar content?**

> â€œå¾ˆå¤šä¸åŒ URL å¯¹åº”ç›¸åŒå†…å®¹ï¼Œæ€ä¹ˆä¼˜åŒ–ï¼Ÿâ€

#### âœ… Answer:

* Detect duplicate content via:

  * **Hashing the HTML** (e.g. SHA-256 of content).
  * **Shingling + MinHash** to detect near-duplicates.
  * **SimHash** for structural similarity.
* Store and compare hashes in a cache/DB.

---

### ğŸ” Q4: **What's the difference between thread and process? When to use each?**

> â€œthread vs process åŒºåˆ«å’Œä½¿ç”¨åœºæ™¯â€

| Aspect        | Thread                     | Process                           |
| ------------- | -------------------------- | --------------------------------- |
| Memory        | Shared                     | Isolated                          |
| Overhead      | Low                        | High                              |
| Best for      | I/O-bound tasks            | CPU-bound tasks                   |
| Python limits | Blocked by GIL (use async) | Not blocked (via multiprocessing) |

#### âœ… Use:

* **ThreadPoolExecutor** for I/O (like web scraping).
* **ProcessPoolExecutor** for CPU (like parsing, ML).

---

### ğŸ” Q5: **How would you benchmark and optimize this?**

> â€œè¿˜éœ€è¦ä½  benchmark ä¸‹ performanceâ€

#### âœ… Answer:

* Log **crawling rate**: URLs per second.
* Measure time taken per URL.
* Profile using:

  * `time.perf_counter()`
  * `concurrent.futures.as_completed()`
* Optimize:

  * Thread pool size
  * Async I/O (e.g. `aiohttp` + `asyncio`)
  * URL deduplication speed
  * Use a real profiler (`cProfile`, `pyinstrument`)

---

### ğŸ” Q6: **What if the helper blocks the event loop (async fails)?**

> â€œcoroutine æ˜¯ lightweight ä½†è¦çœ‹ helper æ˜¯ä¸æ˜¯ blockingâ€

#### âœ… Answer:

* `async def` works only if all I/O is **non-blocking**.
* If `get_urls()` uses `requests` or blocking HTTP:

  * Async wonâ€™t help â€” it **blocks the whole loop**.
* Use `aiohttp` or offload blocking calls:

```python
loop.run_in_executor(None, blocking_get_urls, url)
```

---

### ğŸ” Q7: **What if many URLs have `#` fragments?**

> â€œæœ€åè®©æˆ‘ filter æ‰ # çš„ postfixâ€

#### âœ… Answer:

* Remove fragments using `split('#')[0]`
* Or use `urllib.parse.urldefrag()`:

```python
from urllib.parse import urldefrag
clean_url = urldefrag(url)[0]
```

---

### ğŸ” Q8: **Why use BFS over DFS?**

> â€œå¤§æ¦‚è§£å†³æ–¹æ¡ˆæ˜¯ BFSâ€

#### âœ… Answer:

* **BFS** ensures broader crawl coverage early (like a Googlebot).
* Avoids deep single-path exploration.
* Easier to parallelize with a queue structure.

---

## âœ… Interview Plan Summary

| Time   | Task                                            |
| ------ | ----------------------------------------------- |
| 0â€“5m   | Implement DFS crawler                           |
| 5â€“10m  | Add BFS + dedup                                 |
| 10â€“20m | Make it multi-threaded                          |
| 20â€“30m | Handle `#`, domain check, discuss system design |
