Great â€” letâ€™s now walk through the **â€œDistributed Mode and Medianâ€** system design + algorithm interview question in full detail, incorporating every note from your Anthropic dataset and adding scalable system-level thinking.

This is a **classic distributed data problem** requiring knowledge of:

* MapReduce
* Data partitioning
* Network bottlenecks
* Probabilistic dedup
* Distributed selection (for median)

---

# ðŸ“Š Anthropic System Design Interview: Distributed Mode & Median

---

## ðŸ“Œ Problem Prompt

> **"Given a massive dataset split across N machines, find the mode (most frequent element)."**
> Then as a **follow-up**, find the **median**.
>
> âœ… You are given:

* A cluster of **10 machines**
* Each machine has a local dataset (e.g., millions of integers)
* A fixed **data transmission API** already implemented
* **Network is the bottleneck** â€” avoid multiple machines sending to the same receiver at the same time

---

## âœ… Problem 1: Distributed Mode

### ðŸ§  Definitions

* **Mode**: The number that appears most frequently in a dataset.
* In this case: **all values are unique except for two repeated ones** â€” only two duplicates in the entire distributed dataset.

---

## ðŸ’¡ Key Observations

* There are **only two duplicates** in the entire dataset.
* Those two numbers **may not be on the same machine**.
* So we need a way to **move them together** for detection.
* We need to **minimize bandwidth** and avoid **congestion**.

---

## ðŸ§© Strategy: Hash-Based Shuffling (modulo trick)

> â€œæ¯å°æœºå™¨è¯»å®Œè‡ªå·±çš„æ•°æ®ä¹‹åŽï¼ŒæŠŠæ•°æ¨¡åï¼Œç„¶åŽå‘åˆ°å¯¹åº”çš„æœºå™¨ä¸Šâ€

### ðŸ§  Why this works:

* Each number `x` is assigned to machine `x % 10`
* This guarantees that if two machines contain the **same number**, both will send it to the **same destination**
* Machines **only send to one machine**, and **each machine receives uniformly** â€” no congestion

---

## ðŸ› ï¸ Implementation Plan

### Step 1 â€“ Local Preprocessing

Each machine:

* Scans its local dataset
* Computes `x % 10` for each number
* Sends each number to the corresponding machine

This is similar to a **MapReduce Shuffle step**.

### Step 2 â€“ Local Counting

Each machine:

* Receives a stream of integers
* Maintains a hash map `num â†’ frequency`
* If any number has frequency > 1, it's a duplicate â‡’ possible mode

### Step 3 â€“ Result Aggregation

Each machine returns either:

* A number that appears twice (likely candidate), or
* "No duplicates found"

One coordinator machine aggregates the answers.

---

### ðŸ” Example:

Letâ€™s say we have 3 machines:

```
M1: [1, 3, 5, 1001]
M2: [2, 4, 5, 7]
M3: [6, 8, 1001, 10]
```

We hash each number to `num % 10`:

* `5` â†’ M5
* `1001` â†’ M1

Both `5`s will be sent to M5
Both `1001`s will be sent to M1

Each machine can now locally detect duplicates.

---

## ðŸš« Bandwidth Bottleneck Awareness

> â€œæ•°æ®ä¼ è¾“é€Ÿåº¦ä¼šæ˜¯ç“¶é¢ˆï¼Œå°½é‡ä¸è¦æ‰€æœ‰æœºå™¨éƒ½åŒæ—¶å¾€ä¸€å°æœºå™¨å‘æ•°æ®â€

Using `mod 10` ensures:

* Even distribution
* No more than one send per machine
* No machine becomes a bottleneck receiver

---

## âœ… Result

Each machine independently detects local duplicates, then the coordinator picks the highest-frequency one (mode). In this case, we expect the result to be **either of the two duplicated values**.

---

## ðŸ” Follow-Up: **Distributed Median**

---

## ðŸ“Œ Problem 2: Distributed Median

> Find the **median** (middle element) of a massive dataset split across 10 machines

---

### ðŸ§  Constraints

* Each machine has local data (sorted or not)
* You **cannot send all data to one machine**
* Must reduce **network usage**
* Interface to **send/receive is fixed**

---

### ðŸ’¡ Strategy: Distributed Quickselect / Binary Search

We can treat this like **finding the Kth smallest element** in a distributed setting.

---

## ðŸ§® Step-by-Step Plan

---

### Step 1 â€“ Coordinator chooses a pivot

* Randomly pick a pivot value (from a sample, or uniformly)
* Broadcast to all machines

---

### Step 2 â€“ Each machine returns:

* `count_lt` = count of values < pivot
* `count_eq` = count of values == pivot
* `count_gt` = count of values > pivot

Each machine computes these counts **locally**.

---

### Step 3 â€“ Aggregation

Coordinator aggregates:

* Total `count_lt`
* Total `count_eq`
* Total `count_gt`

Now compare `count_lt` to desired K (median index):

* If `count_lt` > K â†’ repeat on left half
* If `count_lt + count_eq` < K â†’ repeat on right half
* Else â†’ pivot is the median

---

### ðŸ” Repeat Until Converge

Keep narrowing down the range:

* Broadcast new pivot
* Machines send back local counts
* Coordinator updates bounds

This is essentially a **distributed quickselect**.

---

## ðŸš€ Optimization: Sampling for Pivot

* Each machine sends a **random sample** (e.g., 100 numbers)
* Coordinator selects a pivot from combined sample
* Better convergence than random guesses

---

## ðŸ“¦ Result

Median is found after **O(log N)** iterations with **minimal data transfer**, only sending **counts and pivots**, not full arrays.

---

## ðŸ§µ Additional Follow-Up Questions

---

### ðŸ” Q1: How do you minimize network usage?

* Avoid sending raw data â†’ send counts only
* Compress numbers if transmitting samples
* Batch messages if possible

---

### ðŸ” Q2: What if the data isnâ€™t evenly distributed?

* Use **stratified sampling**
* Balance load by breaking largest partitions across multiple machines

---

### ðŸ” Q3: What if data is already sorted?

* Use **merge-style median finding**
* Use **two pointers** (like external k-way merge)

---

### ðŸ” Q4: Can this be done with MapReduce?

âœ… Yes.

For mode:

* Map: `num â†’ 1`
* Reduce: sum counts â†’ max

For median:

* Needs sorting / partial sampling
* Use MapReduce to get distribution
* Do post-processing to find midpoint

---

## âœ… Interview Plan Summary

| Time   | Task                            |
| ------ | ------------------------------- |
| 0â€“5m   | Understand constraints, scale   |
| 5â€“10m  | Propose hash + shuffle for mode |
| 10â€“15m | Handle bandwidth tradeoffs      |
| 15â€“25m | Binary search for median        |
| 25â€“30m | Optimize + answer follow-ups    |

---

